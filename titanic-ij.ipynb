{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-22T14:11:37.053648Z","iopub.execute_input":"2021-05-22T14:11:37.054354Z","iopub.status.idle":"2021-05-22T14:11:37.063468Z","shell.execute_reply.started":"2021-05-22T14:11:37.054300Z","shell.execute_reply":"2021-05-22T14:11:37.062299Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"/kaggle/input/titanic/train.csv\n/kaggle/input/titanic/test.csv\n/kaggle/input/titanic/gender_submission.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Reading Data","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/titanic/train.csv')\n# list(data.columns)\n\ntest_data = pd.read_csv('/kaggle/input/titanic/test.csv')\n# test_data.head()\n\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-22T14:11:37.065762Z","iopub.execute_input":"2021-05-22T14:11:37.066113Z","iopub.status.idle":"2021-05-22T14:11:37.111025Z","shell.execute_reply.started":"2021-05-22T14:11:37.066081Z","shell.execute_reply":"2021-05-22T14:11:37.109748Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n4            5         0       3   \n\n                                                Name     Sex   Age  SibSp  \\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n2                             Heikkinen, Miss. Laina  female  26.0      0   \n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n4                           Allen, Mr. William Henry    male  35.0      0   \n\n   Parch            Ticket     Fare Cabin Embarked  \n0      0         A/5 21171   7.2500   NaN        S  \n1      0          PC 17599  71.2833   C85        C  \n2      0  STON/O2. 3101282   7.9250   NaN        S  \n3      0            113803  53.1000  C123        S  \n4      0            373450   8.0500   NaN        S  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Braund, Mr. Owen Harris</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>Heikkinen, Miss. Laina</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>53.1000</td>\n      <td>C123</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Allen, Mr. William Henry</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"#FIlling NaN values of age with the median age of people with the same salutation\n\nmedian_age = data['Age'].median()\nmedian_age_miss = data[data[\"Name\"].str.contains('Miss.', na=False)]['Age'].median().round()\nmedian_age_mrs = data[data[\"Name\"].str.contains('Mrs.', na=False)]['Age'].median().round()\nmedian_age_mr = data[data[\"Name\"].str.contains('Mr.', na=False)]['Age'].median().round()\nmedian_age_master = data[data[\"Name\"].str.contains('Master.', na=False)]['Age'].median().round()\n\n\ndef fill_age(name_age):\n    \n    name = name_age[0]\n    age = name_age[1]\n    \n    if pd.isnull(age):\n        if 'Mr.' in name:\n            return median_age_mr\n        if 'Mrs.' in name:\n            return median_age_mrs\n        if 'Miss.' in name:\n            return median_age_miss\n        if 'Master.' in name:\n            return median_age_master\n        if 'Ms.' in name:\n            return median_age_miss\n        return median_age\n    else:\n        return age\n\n#writing a function that modifies cabin to include only the first letter\ndef modify_cabin(cabin):\n    if pd.isnull(cabin):\n        return cabin\n    else:\n        return cabin[0]\n        ","metadata":{"execution":{"iopub.status.busy":"2021-05-22T14:11:37.113113Z","iopub.execute_input":"2021-05-22T14:11:37.113427Z","iopub.status.idle":"2021-05-22T14:11:37.135261Z","shell.execute_reply.started":"2021-05-22T14:11:37.113397Z","shell.execute_reply":"2021-05-22T14:11:37.134104Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from sklearn import preprocessing\n\n#Modifiying cabin to include only the first letter as there are too many different values\ndata['Cabin']= data['Cabin'].apply(modify_cabin)\n#Filling NaN values of Cabin with the most frequent value\ndata['Cabin'].fillna(data['Cabin'].mode()[0], inplace = True)\n\n#Filling NaN of age \ndata['Age'] = data[['Name', 'Age']].apply(fill_age,axis=1)\ntest_data['Age'] = test_data[['Name', 'Age']].apply(fill_age,axis=1)\n\n#Filling NaN of Embarked with the most frequent value\ndata['Embarked'].fillna(data['Embarked'].mode()[0], inplace = True)\n\n#Filling NaN of Fare with the median\ntest_data['Fare'].fillna(test_data['Fare'].median(), inplace = True)\n\n\n#Encoding the string labels into int\nsex_le = preprocessing.LabelEncoder()\nticket_le = preprocessing.LabelEncoder()\ncabin_le = preprocessing.LabelEncoder()\nembarked_le = preprocessing.LabelEncoder()\n\ndata['Sex'] = sex_le.fit_transform(data['Sex'])\ndata['Ticket'] = ticket_le.fit_transform(data['Ticket'])\ndata['Cabin'] = cabin_le.fit_transform(data['Cabin'])\ndata['Embarked'] = embarked_le.fit_transform(data['Embarked'])\n\ntest_data['Sex'] = sex_le.fit_transform(test_data['Sex'])\ntest_data['Ticket'] = ticket_le.fit_transform(test_data['Ticket'])\ntest_data['Cabin'] = cabin_le.fit_transform(test_data['Cabin'])\ntest_data['Embarked'] = embarked_le.fit_transform(test_data['Embarked'])\n\n#Splitting data into train and valid\nfrom sklearn.model_selection import train_test_split\ntrain_data,valid_data = train_test_split(data)\n\n#Choosing only the requiired features (Ex. Name and passenger id are unique for everyone so we can ignore that)\nX_train=train_data[['Pclass', 'Sex', 'Age', 'SibSp', 'Ticket', 'Fare','Cabin','Parch','Embarked']] \nX_test = test_data[['Pclass', 'Sex', 'Age', 'SibSp',  'Ticket', 'Fare','Cabin','Parch','Embarked']]\ny_train=train_data['Survived']\nX_valid = valid_data[['Pclass', 'Sex', 'Age', 'SibSp', 'Ticket', 'Fare','Cabin','Parch','Embarked']]\ny_valid=valid_data['Survived'] \n\n#Scaling the data\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\nX_valid = sc.transform(X_valid)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T14:11:37.136729Z","iopub.execute_input":"2021-05-22T14:11:37.137186Z","iopub.status.idle":"2021-05-22T14:11:37.204330Z","shell.execute_reply.started":"2021-05-22T14:11:37.137138Z","shell.execute_reply":"2021-05-22T14:11:37.203165Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Random Hyperparameter Grid\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\nn_estimators = [100*x for x in range(1,11)]\nmax_features = ['auto', 'sqrt']\nmax_depth = [10*x for x in range(1,11)]\nmax_depth.append(None)\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2, 4]\nbootstrap = [True, False]\nclass_weights= [\"balanced\",\"balanced_subsample\",None]\n\n#Creating a random grid with the above parameters\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap,\n              'class_weight': class_weights}\n\n\n#Creating a Random Forest Model and doing a Random Grid Search\nclf=RandomForestClassifier()\nclf_random = RandomizedSearchCV(estimator = clf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\nclf_random.fit(X_train,y_train)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-22T14:11:37.206028Z","iopub.execute_input":"2021-05-22T14:11:37.206338Z","iopub.status.idle":"2021-05-22T14:13:56.784964Z","shell.execute_reply.started":"2021-05-22T14:11:37.206308Z","shell.execute_reply":"2021-05-22T14:13:56.784237Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Fitting 3 folds for each of 100 candidates, totalling 300 fits\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(), n_iter=100,\n                   n_jobs=-1,\n                   param_distributions={'bootstrap': [True, False],\n                                        'class_weight': ['balanced',\n                                                         'balanced_subsample',\n                                                         None],\n                                        'max_depth': [10, 20, 30, 40, 50, 60,\n                                                      70, 80, 90, 100, None],\n                                        'max_features': ['auto', 'sqrt'],\n                                        'min_samples_leaf': [1, 2, 4],\n                                        'min_samples_split': [2, 5, 10],\n                                        'n_estimators': [100, 200, 300, 400,\n                                                         500, 600, 700, 800,\n                                                         900, 1000]},\n                   random_state=42, verbose=2)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Analysis of features","metadata":{}},{"cell_type":"code","source":"#Analysing the Importance of each of the features in classification\nfeature_imp = pd.Series(clf_random.best_estimator_.feature_importances_, index=['Pclass', 'Sex', 'Age', 'SibSp', 'Ticket', 'Fare','Cabin','Parch','Embarked']).sort_values(ascending=False)\nprint(feature_imp)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T14:13:56.786873Z","iopub.execute_input":"2021-05-22T14:13:56.787169Z","iopub.status.idle":"2021-05-22T14:13:56.855576Z","shell.execute_reply.started":"2021-05-22T14:13:56.787138Z","shell.execute_reply":"2021-05-22T14:13:56.854573Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Sex         0.271542\nTicket      0.197449\nFare        0.172151\nAge         0.145293\nPclass      0.078264\nSibSp       0.045369\nCabin       0.039244\nParch       0.026245\nEmbarked    0.024443\ndtype: float64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Train and Validation Accuracies","metadata":{}},{"cell_type":"code","source":"\nfrom sklearn.metrics import confusion_matrix,classification_report\n\ny_train_pred = clf_random.predict(X_train)\nprint(classification_report(y_train,y_train_pred))\n\ny_valid_pred = clf_random.predict(X_valid)\nprint(classification_report(y_valid, y_valid_pred))","metadata":{"execution":{"iopub.status.busy":"2021-05-22T14:13:56.857180Z","iopub.execute_input":"2021-05-22T14:13:56.857740Z","iopub.status.idle":"2021-05-22T14:13:57.038023Z","shell.execute_reply.started":"2021-05-22T14:13:56.857697Z","shell.execute_reply":"2021-05-22T14:13:57.037039Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.96      0.98      0.97       414\n           1       0.96      0.94      0.95       254\n\n    accuracy                           0.96       668\n   macro avg       0.96      0.96      0.96       668\nweighted avg       0.96      0.96      0.96       668\n\n              precision    recall  f1-score   support\n\n           0       0.85      0.87      0.86       135\n           1       0.80      0.76      0.78        88\n\n    accuracy                           0.83       223\n   macro avg       0.82      0.82      0.82       223\nweighted avg       0.83      0.83      0.83       223\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Predicting on Testing Data and generating csv\n","metadata":{}},{"cell_type":"code","source":"y_test= clf_random.predict(X_test)\nimport csv\n\nwith open('submission.csv', mode='w') as submission_file:\n    submission_writer = csv.writer(submission_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n\n    submission_writer.writerow(['PassengerId', 'Survived'])\n    for i in range(len(y_test)):\n        submission_writer.writerow([test_data['PassengerId'][i],y_test[i]])","metadata":{"execution":{"iopub.status.busy":"2021-05-22T14:13:57.039447Z","iopub.execute_input":"2021-05-22T14:13:57.039755Z","iopub.status.idle":"2021-05-22T14:13:57.128293Z","shell.execute_reply.started":"2021-05-22T14:13:57.039724Z","shell.execute_reply":"2021-05-22T14:13:57.126807Z"},"trusted":true},"execution_count":17,"outputs":[]}]}